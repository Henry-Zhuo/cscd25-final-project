{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\".\")\n",
    "import dimen_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load community embedding\n",
    "vectors, metadata = dimen_generation.load_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all pairs of similar communities\n",
    "dimen_generator = dimen_generation.DimenGenerator(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds the dimension for each given seed and dimen_names from seeds_dimen_name_pairs, then stores them in given filename\n",
    "# Lightly modified from code from https://github.com/CSSLab/social-dimensions\n",
    "def find_dimensions(seeds_dimen_name_pairs, scores_file_name):\n",
    "    seeds = [x[0] for x in seeds_dimen_name_pairs]\n",
    "    dimen_names = [x[1] for x in seeds_dimen_name_pairs]\n",
    "    \n",
    "    dimensions = dimen_generator.generate_dimensions_from_seeds(seeds)\n",
    "\n",
    "    for name, dimen in zip(dimen_names, dimensions):\n",
    "        print(\"Dimension %s:\" % name)\n",
    "        print(\"\\tSeed: %s\" % dimen[\"seed\"])\n",
    "        print(\"\\tFound seeds:\")\n",
    "        for c1, c2 in zip(dimen[\"left_comms\"], dimen[\"right_comms\"]):\n",
    "            print(\"\\t\\t%s -> %s\" % (c1, c2))\n",
    "\n",
    "    # Calculate scores for communities\n",
    "    scores = dimen_generation.score_embedding(vectors, zip(dimen_names, dimensions))\n",
    "    print(scores.head(5))\n",
    "\n",
    "    # Save the scores to a csv\n",
    "    scores.to_csv(scores_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will attempt to find seed values that will help us identify the dimensions for whether subreddits are American focused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following use subreddits not in the dataset used to generate the vectors\n",
    "#americanness_seed = [(\"MURICA\", \"Canadia\")]\n",
    "#americanness_seed = [(\"AskAnAmerican\", \"AskACanadian\")]\n",
    "#americanness_seed = [(\"buildapc\", \"buildapccanada\")]\n",
    "\n",
    "find_dimensions(\n",
    "    [((\"personalfinance\", \"PersonalFinanceCanada\"), \"americanness1\")],\n",
    "    \"finance_seeded_americanness_scores.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: Lots of Canadian 2nd elements, occasionally they aren't exactly Canadian specific. Ex: ShieldAndroidTV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All seeds are focused on finding American subreddits\n",
    "finance_seed = ((\"personalfinance\", \"PersonalFinanceCanada\"), \"finance\")\n",
    "news_seed = ((\"news\", \"worldnews\"), \"news\")\n",
    "\n",
    "find_dimensions(\n",
    "    [finance_seed, news_seed],\n",
    "    \"finance_news_americanness_scores.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: Results for first dimension are the same, which is to be expected since the dimension generator tes dimensions for each seed value, independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last set of seeds before trying another approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consulting https://www.reddit.com/r/ListOfSubreddits/wiki/listofsubreddits/ to an idea of subreddits\n",
    "city1_seed = ((\"nyc\", \"toronto\"), \"city1\")\n",
    "\n",
    "find_dimensions(\n",
    "    [city1_seed],\n",
    "    \"city_americanness_scores.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon_go_seed = ((\"pokemongoNYC\", \"pokemongo\"), \"pokemon_go\")\n",
    "\n",
    "find_dimensions(\n",
    "    [pokemon_go_seed],\n",
    "    \"city_americanness_scores.csv\"\n",
    ")\n",
    "\n",
    "# Some results don't seem to be related to how American the subreddits are.\n",
    "# Ex: CabaloftheBuildsmiths -> pcmasterrace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r/seattle isn't in the dataset but SeattleWA is\n",
    "city2_seed = ((\"SeattleWA\", \"vancouver\"), \"city2\")\n",
    "\n",
    "find_dimensions(\n",
    "    [city2_seed],\n",
    "    \"city2_americanness_scores.csv\"\n",
    ")\n",
    "\n",
    "# Generated seed onguardforthee -> britishcolumbia despite both being Canadian subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On the use of dimensions for finding Americanness\n",
    "\n",
    "One factor making this difficult is that there aren't many subreddits focused on content not related to the U.S., with a U.S. focused counterpart. For example, r/india could be the 2nd value of the seed, but r/america is a small subreddit and isn't included in the vectors.\n",
    "\n",
    "Another factor is the errorneous generated seeds, such as `onguardforthee -> britishcolumbia` despite both being Canadian focused subreddits, and thus not American. This could be a product of seed being bad, a bit unlikely since Seattle is an American city while Vancouver is a Canadian city, and there is talk about how Vancouver is sometimes used as a filming location to give the impression of being in Seattle[2]. Another possibility is that how American focused a subreddit is, isn't ordered along some dimension, but rather exists in clusters.\n",
    "\n",
    "I see a couple ways to go forward:\n",
    "1. Attempt some kind of clustering or classification method on the vectors, to determine if subreddits are focused on countries that aren't the U.S.\n",
    "    - Requires manual labelling of at least some of the subreddits. Input features can be the Word2Vec vectors for each subreddit, from \"Quantifying social organization and political polarization in online platforms\"[1]\n",
    "2. Focus on individual posts instead of subreddits\n",
    "    - Requires generating dimensions from seeds, and the seeds would be words that are typically used by American content or users, and words that are not. Not entirely undoable, but finding words typically used by Americans requires data analysis on a dataset of words used\n",
    "\n",
    "Note: The Word2Vec space is community \"contexts\" to user \"words\". A subreddit is catered towards users of the same metric, if more users of that metric, comment in that subreddit. The assumption is that users that are similar by some metric, such as having a similar age, will often comment in similar subreddits, and not often if they aren't similar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cscd25",
   "language": "python",
   "name": "cscd25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\".\")\n",
    "import dimen_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load community embedding\n",
    "vectors, metadata = dimen_generation.load_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all pairs of similar communities\n",
    "dimen_generator = dimen_generation.DimenGenerator(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds the dimension for each given seed and dimen_names from seeds_dimen_name_pairs, then stores them in given filename\n",
    "# Lightly modified from code from https://github.com/CSSLab/social-dimensions\n",
    "def find_dimensions(seeds_dimen_name_pairs, scores_file_name):\n",
    "    seeds = [x[0] for x in seeds_dimen_name_pairs]\n",
    "    dimen_names = [x[1] for x in seeds_dimen_name_pairs]\n",
    "    \n",
    "    dimensions = dimen_generator.generate_dimensions_from_seeds(seeds)\n",
    "\n",
    "    for name, dimen in zip(dimen_names, dimensions):\n",
    "        print(\"Dimension %s:\" % name)\n",
    "        print(\"\\tSeed: %s\" % dimen[\"seed\"])\n",
    "        print(\"\\tFound seeds:\")\n",
    "        for c1, c2 in zip(dimen[\"left_comms\"], dimen[\"right_comms\"]):\n",
    "            print(\"\\t\\t%s -> %s\" % (c1, c2))\n",
    "\n",
    "    # Calculate scores for communities\n",
    "    scores = dimen_generation.score_embedding(vectors, zip(dimen_names, dimensions))\n",
    "    print(scores.head(5))\n",
    "\n",
    "    # Save the scores to a csv\n",
    "    scores.to_csv(scores_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN and OPTICS are clustering algorithms supporting uneven cluster sizes.[4] By setting a value for eps, DBSCAN clusters can be extracted from OPTICS clusters. eps limits the distance between sample points, for them to be considered part of the same cluster, and strongly influences the clusters generated by DBSCAN. Having communities not be labelled is not of much concern, since it means they aren't similar enough in their users, to other communities. Thus how US-centric or non-US-centric their users are, is not particularly strong.\n",
    "\n",
    "Maximizing the number of communities labelled, allows us to have a better chance at labelling clusters as US-centric or not, and having more clusters helps with being more precise about whether a community is US-centric or not. However, we cannot solely maximize the number of communities labelled, otherwise we'd put everything into a single cluster, which is not informative.\n",
    "\n",
    "\n",
    "\n",
    "Additional sources:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/clustering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import OPTICS\n",
    "\n",
    "# Using squared Euclidean to avoid normalizing by variance of vectors\n",
    "optics_clustering = OPTICS(metric='sqeuclidean')\n",
    "optics_clustering.fit(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Help from https://scikit-learn.org/stable/auto_examples/cluster/plot_optics.html#sphx-glr-auto-examples-cluster-plot-optics-py\n",
    "\n",
    "from sklearn.cluster import cluster_optics_dbscan\n",
    "\n",
    "\"\"\" Generates DBSCAN cluster and info about it,\n",
    "    given OPTICS cluster and eps value for DBSCAN \"\"\"\n",
    "def cluster_for_eps(optics_cluster, eps):\n",
    "    dbscan_labels = cluster_optics_dbscan(\n",
    "        reachability=optics_clustering.reachability_,\n",
    "        core_distances=optics_clustering.core_distances_,\n",
    "        ordering=optics_clustering.ordering_,\n",
    "        eps=eps,\n",
    "    )\n",
    "\n",
    "    # Checking for non-0 labels, since we want more than 1 cluster.\n",
    "    # The first cluster has label 0.\n",
    "    cluster_labels = [label for label in dbscan_labels if label > 0]\n",
    "    samples_labelled = len(cluster_labels)\n",
    "    cluster_count = 0\n",
    "    if samples_labelled != 0:\n",
    "        cluster_count = max(cluster_labels)\n",
    "\n",
    "    return {\n",
    "        \"eps\": eps,\n",
    "        \"labels\": dbscan_labels,\n",
    "        # This is samples labelled that aren't in the first cluster,\n",
    "        # or considered noise.\n",
    "        \"samples_labelled\": samples_labelled,\n",
    "        \"cluster_count\": cluster_count + 1,\n",
    "    }\n",
    "\n",
    "def print_dbscan_cluster_info(dbscan_cluster_info):\n",
    "    print(f'eps used: {dbscan_cluster_info[\"eps\"]}')\n",
    "    print(f'\\tSamples labelled: {dbscan_cluster_info[\"samples_labelled\"]}')\n",
    "    print(f'\\tCluster count: {dbscan_cluster_info[\"cluster_count\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATIONS = 100\n",
    "EPS_RATE = 50\n",
    "found_max_sample_labelling = False\n",
    "\n",
    "for i in range(1, ITERATIONS):\n",
    "    eps = i / EPS_RATE\n",
    "    dbscan_info = cluster_for_eps(optics_clustering, eps)\n",
    "    if not found_max_sample_labelling:\n",
    "        max_sample_labelled_clustering = dbscan_info\n",
    "        found_max_sample_labelling = True\n",
    "    elif dbscan_info[\"samples_labelled\"] > max_sample_labelled_clustering[\"samples_labelled\"]:\n",
    "        max_sample_labelled_clustering = dbscan_info\n",
    "\n",
    "print_dbscan_cluster_info(max_sample_labelled_clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An eps of 0.54 gave the most number of communities labelled as part of a cluster, with a total of 174 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some communities are US-centric, such as r/nfl, with its focus being the NFL, a US sports league. On the other end, communities such as r/india and r/vancouver, are focused on topics that are unrelated to the US. However, there are communities that aren't specifically focused on US topics, such as r/pics. These are of interest, since they more closely represent what Reddit (at least the English speaking demographic of it) as a whole, are interested in. If we find more communities not focused on US topics, but the users are similar to that of communities that are focused on US topics, then the communities will be close together in the vector space and we can conclude that users are fairly focused on US topics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cscd25",
   "language": "python",
   "name": "cscd25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
